{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Starting batch job...\")\n",
    "    # Run the Bytewax batch dataflow\n",
    "    # subprocess.run([\".\\Venev_realtimeRag\\Scripts\\activate\"], check=True, capture_output=True, text=True)\n",
    "    python_executable = sys.executable\n",
    "    # subprocess.run([python_executable, \"-m\", \"bytewax.run\", \"ingestion_pipeline:build_batch_dataflow\"], check=True, capture_output=True, text=True)\n",
    "    process = subprocess.Popen(\n",
    "            [python_executable, \"-m\", \"bytewax.run\", \"ingestion_pipeline:build_batch_dataflow\"],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "    # Read stdout line by line\n",
    "    for line in process.stdout:\n",
    "        print(line.strip())\n",
    "\n",
    "    # Read stderr line by line\n",
    "    for line in process.stderr:\n",
    "        print(line.strip())\n",
    "\n",
    "    process.wait()  \n",
    "    print(\"Batch job completed successfully.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Batch job failed: {e}\")\n",
    "    print(f\"Error output: {e.stderr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apscheduler.schedulers.blocking import BlockingScheduler # from apscheduler.schedulers.blocking import BlockingScheduler\n",
    "from apscheduler.triggers.cron import CronTrigger\n",
    "import subprocess\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def run_batch_job():\n",
    "    \"\"\"Function to run the Bytewax batch dataflow.\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting batch job...\")\n",
    "        # Run the Bytewax batch dataflow\n",
    "        subprocess.run([\"python\", \"-m\", \"bytewax.run\", \"ingestion_pipeline:build_batch_dataflow\"], check=True)\n",
    "        logger.info(\"Batch job completed successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logger.error(f\"Batch job failed: {e}\")\n",
    "\n",
    "# Create a scheduler\n",
    "scheduler = BlockingScheduler()\n",
    "\n",
    "# trigger = CronTrigger(year=\"*\", month=\"*\", day=\"*\", hour=\"*\", minute=\"0\", second=\"10\")\n",
    "# Schedule the job to run daily at 2 AM\n",
    "scheduler.add_job(run_batch_job, 'interval', seconds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the scheduler\n",
    "\n",
    "logger.info(\"Starting scheduler...\")\n",
    "scheduler.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from fire import Fire\n",
    "\n",
    "from streaming_pipeline import constants, initialize\n",
    "from streaming_pipeline.embeddings import EmbeddingModelSingleton\n",
    "from streaming_pipeline.qdrant import build_qdrant_client\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def search(query_string: str):\n",
    "    \"\"\"\n",
    "    Searches for the closest points to the given query string in the vector database.\n",
    "\n",
    "    Args:\n",
    "        query_string (str): The query string to search for.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    initialize()\n",
    "\n",
    "    client = build_qdrant_client()\n",
    "    model = EmbeddingModelSingleton()\n",
    "\n",
    "    query_embedding = model(query_string, to_list=True)\n",
    "\n",
    "    hits = client.search(\n",
    "        collection_name=constants.VECTOR_DB_OUTPUT_COLLECTION_NAME,\n",
    "        query_vector=query_embedding,\n",
    "        limit=2,  # Return 5 closest points\n",
    "    )\n",
    "    \n",
    "    for hit in hits:\n",
    "        logger.info(hit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"What did Sangamo announced today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "from bytewax.dataflow import Dataflow\n",
    "from bytewax.testing import TestingSource\n",
    "from pydantic import parse_obj_as\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "from streaming_pipeline import mocked\n",
    "# from streaming_pipeline.alpaca_batch import AlpacaNewsBatchInput\n",
    "# from streaming_pipeline.alpaca_stream import AlpacaNewsStreamInput\n",
    "from streaming_pipeline.embeddings import EmbeddingModelSingleton\n",
    "from streaming_pipeline.models import NewsArticle, Document\n",
    "from streaming_pipeline.qdrant import QdrantVectorOutput\n",
    "\n",
    "from bytewax import operators as op\n",
    "\n",
    "model = EmbeddingModelSingleton(cache_dir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input(is_input_mocked: bool = True,):\n",
    "    if is_input_mocked:\n",
    "        return TestingSource(mocked.financial_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output(model: EmbeddingModelSingleton, in_memory: bool = False):\n",
    "    if in_memory:\n",
    "        return QdrantVectorOutput(\n",
    "            vector_size=model.max_input_length,\n",
    "            client=QdrantClient(\":memory:\"),\n",
    "        )\n",
    "    else:\n",
    "        return QdrantVectorOutput(\n",
    "            vector_size=model.max_input_length,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import TypeAdapter\n",
    "\n",
    "article_adapter = TypeAdapter(List[NewsArticle])\n",
    "\n",
    "flow = Dataflow(\"alpaca_news_input\")\n",
    "\n",
    "alpaca_news_input = op.input(\"input\", flow, build_input())\n",
    "\n",
    "article_to_class = op.flat_map(\"class_to_article\", alpaca_news_input, lambda messages: article_adapter.validate_python(messages))\n",
    "_ = op.inspect(\"articles\", article_to_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = op.map(\"document\", article_to_class, lambda article: article.to_document())\n",
    "_ = op.inspect(\"inspect_document\", document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_chunks = op.map(\"chunks\", document, lambda document: document.compute_chunks(model))\n",
    "_ = op.inspect(\"inspect_chunks\", compute_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_embeddings = op.map(\"embeddings\", compute_chunks, lambda document: document.compute_embeddings(model)) # flow.map(lambda document: document.compute_embeddings(model))\n",
    "_ = op.inspect(\"inspect_embeddings\", compute_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = op.output(\"output\", compute_embeddings, build_output(model)) # flow.output(\"output\", _build_output(model, in_memory=debug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bytewax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pathlib import Path\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "class CustomEmbeddings(Embeddings):\n",
    "    \"\"\"\n",
    "    Wrapper for the custom embedding model to make it compatible with LangChain.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the custom embeddings wrapper.\n",
    "        \n",
    "        Args:\n",
    "            model_id: The identifier of the pre-trained transformer model\n",
    "            max_input_length: Maximum length of input text to tokenize\n",
    "            device: Device to use for running the model\n",
    "            cache_dir: Directory to cache the pre-trained model files\n",
    "        \"\"\"\n",
    "        \n",
    "        self.embedding_model = EmbeddingModelSingleton()\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of documents.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            List of embeddings, one per text\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            embedding = self.embedding_model(text, to_list=True)\n",
    "            embeddings.append(embedding)\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Generate embedding for a single query text.\n",
    "        \n",
    "        Args:\n",
    "            text: Text string to embed\n",
    "            \n",
    "        Returns:\n",
    "            Query embedding\n",
    "        \"\"\"\n",
    "        return self.embedding_model(text, to_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_qdrant import QdrantVectorStore, RetrievalMode\n",
    "from typing import List, Dict\n",
    "from langchain_core.documents import Document as LangChainDocument\n",
    "\n",
    "class NewsRAGSystem:\n",
    "    def __init__(\n",
    "        self,\n",
    "        qdrant_url: Optional[str] = None,\n",
    "        qdrant_api_key: Optional[str] = None,\n",
    "        openai_api_key: str = \"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\",\n",
    "        retrieval_mode: RetrievalMode = RetrievalMode.DENSE,\n",
    "        content_payload_key: str = \"text\",\n",
    "        metadata_payload_key: str = \"payload\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system with Qdrant and OpenAI credentials.\n",
    "        \n",
    "        Args:\n",
    "            qdrant_url: Qdrant Cloud URL\n",
    "            qdrant_api_key: Qdrant Cloud API key\n",
    "            collection_name: Name of the collection in Qdrant\n",
    "            openai_api_key: OpenAI API key\n",
    "        \"\"\"\n",
    "        # Initialize Qdrant client\n",
    "        self.client = build_qdrant_client(api_key=qdrant_api_key, url=qdrant_url)\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.embeddings = CustomEmbeddings()\n",
    "        \n",
    "        # Initialize vector store\n",
    "        self.vectorstore = QdrantVectorStore(\n",
    "            client=self.client,\n",
    "            collection_name=constants.VECTOR_DB_OUTPUT_COLLECTION_NAME,\n",
    "            embedding=self.embeddings,\n",
    "            content_payload_key=content_payload_key,\n",
    "            metadata_payload_key=metadata_payload_key,\n",
    "            retrieval_mode=retrieval_mode\n",
    "        )\n",
    "        \n",
    "        # Initialize retriever\n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",  # Using MMR for better diversity in results\n",
    "            search_kwargs={\"k\": 3}\n",
    "        )\n",
    "        \n",
    "        # Initialize LLM\n",
    "        self.llm = ChatOpenAI(\n",
    "            model_name=\"gpt-4o\",\n",
    "            openai_api_key=openai_api_key,\n",
    "            base_url=\"https://openrouter.ai/api/v1\"\n",
    "        )\n",
    "        \n",
    "        # Create prompt template\n",
    "        self.prompt = ChatPromptTemplate.from_template(\"\"\"You are a helpful assistant that answers questions about financial news articles.\n",
    "        Use the following pieces of context to answer the question at the end.\n",
    "        If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Helpful Answer:\"\"\")\n",
    "        \n",
    "        # Initialize RAG chain\n",
    "        self.chain = self._create_rag_chain()\n",
    "    \n",
    "    def _format_docs(self, docs: List[LangChainDocument]) -> str:\n",
    "        \"\"\"Format documents into a string.\"\"\"\n",
    "        print(\"########### The Documents ########### :: \", docs)\n",
    "        return \"\\n\\n\".join(f'Content {i}:\\n{doc.page_content}' for i, doc in enumerate(docs))\n",
    "    \n",
    "    def _create_rag_chain(self):\n",
    "        \"\"\"\n",
    "        Create the RAG chain using the latest LangChain syntax.\n",
    "        \"\"\"\n",
    "        # Define the RAG chain\n",
    "        chain = (\n",
    "            RunnableParallel(\n",
    "                {\"context\": self.retriever | self._format_docs, \"question\": RunnablePassthrough()}\n",
    "            )\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        return chain\n",
    "    \n",
    "    def query(self, question: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Query the RAG system with a question.\n",
    "        \n",
    "        Args:\n",
    "            question: The question to ask\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing the answer and source documents\n",
    "        \"\"\"\n",
    "        # Get the answer\n",
    "        answer = self.chain.invoke(question)\n",
    "        \n",
    "        # Get source documents\n",
    "        docs = self.retriever.invoke(question)\n",
    "        \n",
    "        # Format source documents\n",
    "        sources = []\n",
    "        for doc in docs:\n",
    "            source = {\n",
    "                \"headline\": doc.metadata.get(\"headline\", \"N/A\"),\n",
    "                \"url\": doc.metadata.get(\"url\", \"N/A\"),\n",
    "                \"created_at\": doc.metadata.get(\"created_at\", \"N/A\"),\n",
    "                \"symbols\": doc.metadata.get(\"symbols\", []),\n",
    "                \"author\": doc.metadata.get(\"author\", \"N/A\")\n",
    "            }\n",
    "            sources.append(source)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": sources\n",
    "        }\n",
    "    \n",
    "    def query_by_filters(\n",
    "        self,\n",
    "        question: str,\n",
    "        symbols: List[str] = None,\n",
    "        date_from: str = None,\n",
    "        date_to: str = None\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Query with additional filters for symbols and date range.\n",
    "        \n",
    "        Args:\n",
    "            question: The question to ask\n",
    "            symbols: List of stock symbols to filter by\n",
    "            date_from: Start date in ISO format\n",
    "            date_to: End date in ISO format\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing the answer and filtered source documents\n",
    "        \"\"\"\n",
    "        # Build filter conditions\n",
    "        filter_conditions = {}\n",
    "        \n",
    "        if symbols:\n",
    "            filter_conditions[\"symbols\"] = {\"$in\": symbols}\n",
    "            \n",
    "        if date_from or date_to:\n",
    "            filter_conditions[\"created_at\"] = {}\n",
    "            if date_from:\n",
    "                filter_conditions[\"created_at\"][\"$gte\"] = date_from\n",
    "            if date_to:\n",
    "                filter_conditions[\"created_at\"][\"$lte\"] = date_to\n",
    "        \n",
    "        # Update retriever search parameters\n",
    "        self.retriever.search_kwargs[\"filter\"] = filter_conditions\n",
    "        \n",
    "        # Get results\n",
    "        result = self.query(question)\n",
    "        \n",
    "        # Reset retriever search parameters\n",
    "        self.retriever.search_kwargs.pop(\"filter\", None)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the RAG system\n",
    "rag_system = NewsRAGSystem()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic query\n",
    "question = \"What was the FDA designation given to Sangamo Therapeutics for their Fabry Disease treatment?\"\n",
    "result = rag_system.query(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_system.vectorstore.similarity_search(\n",
    "    \"What was the FDA designation given to Sangamo Therapeutics for their Fabry Disease treatment?\", k=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
